{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WDoI4oTmWBb"
      },
      "source": [
        "# LinkedIn Post Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjmfDvGAqPtx"
      },
      "source": [
        "Due to frequent changes to the layout of LinkedIn, much of the code from the online tutorials was no longer valid. And Linkedin's anti-crawl mechanism is too strict, so I can't find the interface for scraping. Here are a few ways I've tried and gotten stuck:\n",
        "\n",
        "1. I referenced this [code](https://github.com/christophe-garon/Linkedin-Post-Scraper). But the errors reported during the craw and the main problem is: After logging in to the website, the home page of the website is displayed, but the post search page set in the script cannot be loaded.\n",
        "\n",
        "2. I tried [this](https://stevesie.com/apps/linkedin-apidownload) on YouTube, and downloaded the source code of the page in HAR format, but it cannot be parsed.\n",
        "I installed a Chrome Web Scraper [plug-in](https://chrome.google.com/webstore/detail/web-scraper-free-web-scra/jnhgnonknehpejjnehehllkliplmbmhn?hl=en), but it cannot be cycled for acquisition. I need manually select text areas one by one.\n",
        "\n",
        "3. I tried [Octoparse](https://www.octoparse.com/tutorial-7/scrape-post-from-linkedin), but there was a problem with the simulated login session: I needed to enter the cell phone verification code and it showed \"cell phone number was incorrect\".\n",
        "\n",
        "I finally adopted the first method, fixed the code and successfully crawled the real-time posts in last several hours. I used the following tools:\n",
        "\n",
        "* Selenium: This tool works in conjunction with ChromeDriver to perform our desired functions like clicking links and scrolling. It’s rather cool watching the program run because it appears as though someone is control of the screen.\n",
        "* ChromeDriver: This tool is like the middle man between Selenium and Google Chrome, which allows everything to run smoothly.\n",
        "* Beautiful Soup: This is Python package that will allow us to find and access the various Linkedin elements that we would like to collect. It will scour through the page’s source code finding all of the tags that we instruct it to.\n",
        "\n",
        "We originally planned to crawl 3-month historical posts published between 2021-11-15 and 2022-02-15. This idea failed as we can only get real-time data with totally 276 posts and 2231 words. \n",
        "\n",
        "However, these text data can help us understand what the themes are when user mentioned meta, what noise there would be, and whether there may be a sentimental tendency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8J6lO9nmVnH"
      },
      "outputs": [],
      "source": [
        "# required installs (i.e. pip3 install in terminal): pandas, selenium, bs4\n",
        "import math\n",
        "import sys\n",
        "import time\n",
        "import traceback\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from bs4 import Tag\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver import ActionChains\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "\n",
        "from selenium import webdriver"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gazp-4N5p_JP",
        "outputId": "5202b9cb-c92e-49a7-d04e-412f1b135f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.39)] [Connected to cloud.r-proj\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "\r                                                                               \rHit:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Reading package lists... Done\n",
            "E: dpkg was interrupted, you must manually run 'dpkg --configure -a' to correct the problem. \n",
            "cp: cannot stat '/usr/lib/chromium-browser/chromedriver': No such file or directory\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.7/dist-packages (4.3.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.7/dist-packages (from selenium) (0.9.2)\n",
            "Requirement already satisfied: urllib3[secure,socks]~=1.26 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.26.11)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.7/dist-packages (from selenium) (0.21.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Requirement already satisfied: async-generator>=1.9 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.7/dist-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.6.15)\n",
            "Requirement already satisfied: pyOpenSSL>=0.14 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (22.0.0)\n",
            "Requirement already satisfied: cryptography>=1.3.4 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (37.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wd = webdriver.Chrome('/content/chromedriver.exe')"
      ],
      "metadata": {
        "id": "dQFlu_-xZdVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "LR_v_gb1PbWY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-Qa6eT9nUc7"
      },
      "outputs": [],
      "source": [
        "# writed the LinkedIn account password into the yaml file\n",
        "# get the info for log in automatically when crawling\n",
        "def get_infos():\n",
        "    infos_dict = {}\n",
        "    try:\n",
        "        with open(\"./credentials.yaml\", encoding=\"utf-8\") as f:\n",
        "            infos_dict = yaml.safe_load(f)\n",
        "        print(infos_dict)\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "        sys.exit()\n",
        "    return (\n",
        "        infos_dict.get(\"username\"),\n",
        "        infos_dict.get(\"password\"),\n",
        "        infos_dict.get(\"filter_settings\"),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYWBRzYLnYaJ"
      },
      "outputs": [],
      "source": [
        "# log in\n",
        "def login(browser, url, username, password):\n",
        "    print(\"*\" * 50 + \" sign on \" + \"*\" * 50)\n",
        "    try:\n",
        "        # Open login page\n",
        "        browser.get(url)\n",
        "\n",
        "        # Enter login info:\n",
        "        elementID = browser.find_element(by=By.ID, value=\"username\")\n",
        "        elementID.send_keys(username)\n",
        "\n",
        "        elementID = browser.find_element(by=By.ID, value=\"password\")\n",
        "        elementID.send_keys(password)\n",
        "        # Note: replace the keys \"username\" and \"password\" with your LinkedIn login info\n",
        "        elementID.submit()\n",
        "    except:  # Prevent duplicate logins\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gp-z1MhsncHi"
      },
      "outputs": [],
      "source": [
        "# verification\n",
        "def verification(browser):\n",
        "    try:\n",
        "        code = input(\"Enter the Verification Code: \")\n",
        "        vcode_input = browser.find_element(By.ID, \"input__email_verification_pin\")\n",
        "        vcode_submit = browser.find_element(By.ID, \"email-pin-submit-button\")\n",
        "        vcode_input.send_keys(code)\n",
        "        vcode_submit.submit()\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAeh-IyOnfu6"
      },
      "outputs": [],
      "source": [
        "# find the keyword\n",
        "def find(browser, target_text, max_times=10):\n",
        "    print(\"*\" * 50 + \" start searching \" + \"*\" * 50)\n",
        "    # Go to the home page and start searching:\n",
        "    times = 0\n",
        "    while True:\n",
        "        try:\n",
        "            search_element_root = None\n",
        "            while search_element_root is None:\n",
        "                search_element_root = browser.find_element(\n",
        "                    by=By.ID, value=\"global-nav-typeahead\"\n",
        "                )\n",
        "                time.sleep(1.5)\n",
        "\n",
        "            search_element = search_element_root.find_elements(By.TAG_NAME, \"input\")[0]\n",
        "            search_element.send_keys(target_text)\n",
        "            ActionChains(browser).key_down(Keys.ENTER).send_keys_to_element(\n",
        "                search_element, \"\"\n",
        "            ).perform()\n",
        "            break\n",
        "        except:\n",
        "            times += 1\n",
        "            time.sleep(1.5)\n",
        "            if times > max_times:\n",
        "                print(\"There seems to be a problem loading the page\")\n",
        "                sys.exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwGUlbI0nkOj"
      },
      "outputs": [],
      "source": [
        "# set the filters\n",
        "def set_filters(browser, filters_settings, max_times=20):\n",
        "    \"\"\"Matching type of filter: 0: by index, 1: by name\"\"\"\n",
        "\n",
        "    print(\"*\" * 50 + \" Set filter conditions \" + \"*\" * 50)\n",
        "    filter_base_name = filters_settings[\"base\"]\n",
        "    filters = filters_settings[\"options\"]\n",
        "    times = 0\n",
        "    while True:\n",
        "        try:\n",
        "            if filter_base_name == \"\" or filter_base_name is None:\n",
        "                break\n",
        "            container = browser.find_element_by_id(\n",
        "                \"search-reusables__filters-bar\"\n",
        "            ).find_element_by_tag_name(\"ul\")\n",
        "            base_options = container.find_elements_by_class_name(\n",
        "                \"search-reusables__primary-filter\"\n",
        "            )\n",
        "            for base_option in base_options:\n",
        "                option = base_option.find_element_by_tag_name(\"button\")\n",
        "                if option.get_property(\"innerText\") == filter_base_name:\n",
        "                    ActionChains(browser).click(option).perform()\n",
        "                    break\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            times += 1\n",
        "            traceback.print_exc()\n",
        "            time.sleep(1.5)\n",
        "            if times > max_times:\n",
        "                sys.exit()\n",
        "\n",
        "    if filters is None or len(filters) == 0:\n",
        "        return\n",
        "    for filter in filters:\n",
        "        times = 0\n",
        "        page_show = False\n",
        "        while True:\n",
        "            try:\n",
        "                container = browser.find_element_by_id(\n",
        "                    \"search-reusables__filters-bar\"\n",
        "                ).find_element_by_tag_name(\"ul\")\n",
        "                filter_element = container.find_element_by_id(\n",
        "                    \"hoverable-outlet-{}-filter-value\".format(filter[\"name\"])\n",
        "                )\n",
        "                root = filter_element.find_element_by_xpath(\"..\")\n",
        "                active_button = root.get_property(\"children\")[1].get_property(\n",
        "                    \"children\"\n",
        "                )[0]\n",
        "                if not page_show:\n",
        "                    ActionChains(browser).click(active_button).perform()\n",
        "                    page_show = True\n",
        "                options = filter_element.find_elements_by_tag_name(\"li\")\n",
        "                option = None\n",
        "                if int(filter.get(\"type\", 0)) == 0:\n",
        "                    option = options[filter[\"value\"]]\n",
        "                else:\n",
        "                    for op in options:\n",
        "                        name_span = op.find_element_by_tag_name(\"span\")\n",
        "                        if name_span.get_property(\"innerText\") == filter[\"value\"]:\n",
        "                            option = op\n",
        "                            break\n",
        "                submit_btn = (\n",
        "                    filter_element.find_element_by_tag_name(\"fieldset\")\n",
        "                    .get_property(\"children\")[-1]\n",
        "                    .get_property(\"children\")[-1]\n",
        "                )\n",
        "                if option is not None:\n",
        "                    # selector = option.find_element_by_tag_name(\"input\")\n",
        "                    # ActionChains(browser).click(selector).perform()\n",
        "                    label = option.find_element_by_tag_name(\"label\")\n",
        "                    ActionChains(browser).click(label).perform()\n",
        "                    ActionChains(browser).click(\n",
        "                        submit_btn\n",
        "                    ).perform()  # submit_btn.click()\n",
        "                break\n",
        "            except:\n",
        "                times += 1\n",
        "                traceback.print_exc()\n",
        "                time.sleep(2)\n",
        "                if times > max_times:\n",
        "                    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbP70UlDnofa"
      },
      "outputs": [],
      "source": [
        "# set trends filters\n",
        "def set_trends_filters(browser):\n",
        "    filter_trends = None\n",
        "    times = 0\n",
        "    while filter_trends is None:\n",
        "        try:\n",
        "            filter_trends = browser.find_element(\n",
        "                by=By.CLASS_NAME, value=\"search-reusables__primary-filter\"\n",
        "            )\n",
        "            filter_trends.click()\n",
        "        except:\n",
        "            time.sleep(1.5)\n",
        "            times += 1\n",
        "            if times > 10:\n",
        "                sys.exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zc8UkAnjnupp"
      },
      "outputs": [],
      "source": [
        "# load all resulte\n",
        "def load_all_result(browser):\n",
        "    print(\"*\" * 50 + \" Load all data \" + \"*\" * 50)\n",
        "    times = 0\n",
        "    while True:\n",
        "        try:\n",
        "            containers = browser.find_element_by_class_name(\n",
        "                \"scaffold-finite-scroll__content\"\n",
        "            )\n",
        "            infos_element = (\n",
        "                browser.find_element_by_id(\"main\")\n",
        "                .find_element_by_class_name(\"search-marvel-srp\")\n",
        "                .find_element_by_tag_name(\"h1\")\n",
        "            )\n",
        "            text = infos_element.text\n",
        "            nums = int(text.split(\".\")[2].split(\"total\")[-1].split(\" \")[1])\n",
        "            pages = int(math.ceil(nums / 10))\n",
        "            while pages + 1 != containers.get_property(\"childElementCount\"):\n",
        "                print(\n",
        "                    f\"pages：{pages}，count: {containers.get_property('childElementCount')}\"\n",
        "                )\n",
        "                browser.execute_script(\n",
        "                    \"window.scrollTo(0, document.body.scrollHeight);\"\n",
        "                )\n",
        "                time.sleep(3)\n",
        "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            time.sleep(1)\n",
        "            times += 1\n",
        "            print(ex)\n",
        "            if times > 10:\n",
        "                sys.exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziAPEgTwnxxZ"
      },
      "outputs": [],
      "source": [
        "# get target contents for test\n",
        "def get_target_contents1(browser):\n",
        "    element_roots = []\n",
        "    for i in range(10):\n",
        "        element_roots = browser.find_elements_by_class_name(\n",
        "            \"scaffold-finite-scroll__content\"\n",
        "        )\n",
        "        if len(element_roots) != 0:\n",
        "            break\n",
        "        time.sleep(2)\n",
        "\n",
        "    for element_root in element_roots:\n",
        "        username = element_root.find_element_by_class_name(\n",
        "            \"feed-shared-actor__name t-14 t-bold hoverable-link-text t-black\"\n",
        "        )\n",
        "        # .find_element_by_tag_name(\"span\").get_property(\"innerText\")\n",
        "        date = element_root.find_element_by_class_name(\n",
        "            \"feed-shared-actor__sub-description t-12 t-normal t-black--light\"\n",
        "        )  # .find_element_by_class_name(\"visually-hidden\").get_property(\"innerText\")\n",
        "        text = element_root.find_element_by_class_name(\"break-words\").get_property(\n",
        "            \"children\"\n",
        "        )[0]\n",
        "        # .get_property(\"children\")[0].get_property(\"innerText\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AuykXssn0qK"
      },
      "outputs": [],
      "source": [
        "def get_target_contents(browser):\n",
        "    print(\"*\" * 50 + \" Crawling content \" + \"*\" * 50)\n",
        "    company_page = browser.page_source\n",
        "    linkedin_soup = bs(company_page.encode(\"utf-8\"), \"html.parser\")\n",
        "    linkedin_soup.prettify()\n",
        "    data_roots = []\n",
        "    for i in range(10):\n",
        "        try:\n",
        "            container = linkedin_soup.find(\n",
        "                \"div\", {\"class\": \"scaffold-finite-scroll__content\"}\n",
        "            )\n",
        "            data_roots = container.find_all(\n",
        "                \"div\", {\"class\": \"ph0 pv0 search-results__no-cluster-container mb2\"}\n",
        "            )\n",
        "            if len(data_roots) != 0:\n",
        "                break\n",
        "        except:\n",
        "            traceback.print_exc()\n",
        "            time.sleep(2)\n",
        "\n",
        "    post_user = []\n",
        "    post_dates = []\n",
        "    post_texts = []\n",
        "    for data_root in data_roots:\n",
        "        roots = data_root.children\n",
        "        if roots is None:\n",
        "            continue\n",
        "        for element_root in roots:\n",
        "            try:\n",
        "                if not isinstance(element_root, Tag):\n",
        "                    continue\n",
        "                infos_root = element_root.find(\n",
        "                    \"div\", {\"class\": \"feed-shared-actor__meta relative\"}\n",
        "                )\n",
        "                if infos_root is None:\n",
        "                    with open(\"log.txt\", \"w\") as f:\n",
        "                        f.write(element_root.strings)\n",
        "                        f.write(\"\\n\")\n",
        "                username = infos_root.find(\"span\", {\"dir\": \"ltr\"}).text\n",
        "                date = infos_root.find(\"span\", {\"class\": \"visually-hidden\"}).text\n",
        "                text = (\n",
        "                    element_root.find(\n",
        "                        \"div\",\n",
        "                        {\n",
        "                            \"class\": \"feed-shared-text relative feed-shared-update-v2__commentary\"\n",
        "                        },\n",
        "                    )\n",
        "                    .find(\"span\", {\"dir\": \"ltr\"})\n",
        "                    .text\n",
        "                )\n",
        "                post_user.append(username)\n",
        "                post_dates.append(date.strip())\n",
        "                post_texts.append(text)\n",
        "            except Exception as ex:\n",
        "                traceback.print_exc()\n",
        "\n",
        "    data = {\"User Name\": post_user, \"Date Posted\": post_dates, \"Post Text\": post_texts}\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc-ct5DLn3o5"
      },
      "outputs": [],
      "source": [
        "def write_to_file(datas, file_name):\n",
        "    print(\"*\" * 50 + \" write to file \" + \"*\" * 50)\n",
        "    df = pd.DataFrame(datas)\n",
        "    now_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.localtime(time.time()))\n",
        "    try:\n",
        "        df.to_csv(\n",
        "            \"{}_{}_posts.csv\".format(file_name, now_time), encoding=\"utf-8\", index=False\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    writer = pd.ExcelWriter(\n",
        "        path=\"{}_{}_posts.xlsx\".format(file_name, now_time), engine=\"xlsxwriter\"\n",
        "    )\n",
        "    df.to_excel(writer, index=False)\n",
        "    writer.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I90Mg1Twn5wK",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# access Webriver\n",
        "target_text = \"meta\"\n",
        "url = \"https://www.linkedin.com/login?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin\"\n",
        "base_url = \"https://www.linkedin.com/feed/\"\n",
        "username, password, filter_settings = get_infos()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform Web Scraping"
      ],
      "metadata": {
        "id": "4vbKNlELPi4B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wApdwVxC2IBO"
      },
      "source": [
        "### login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "a3iMxBph2IBO"
      },
      "outputs": [],
      "source": [
        "login(browser, url, username, password)  # sign in\n",
        "while not browser.current_url.startswith(base_url):\n",
        "    time.sleep(2)\n",
        "find(browser, target_text)  # find target text\n",
        "# set_trends_filters(browser)  # only get content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTt2nfMS2IBO"
      },
      "source": [
        "### filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "wRaB8A_r2IBP"
      },
      "outputs": [],
      "source": [
        "set_filters(browser, filter_settings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwOaGE0q2IBP"
      },
      "source": [
        "### scroll to get all pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djs8mO8z2IBP"
      },
      "outputs": [],
      "source": [
        "containers = browser.find_element_by_class_name(\"scaffold-finite-scroll__content\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KqoD3qW2IBQ"
      },
      "outputs": [],
      "source": [
        "infos_element = (\n",
        "    browser.find_element_by_id(\"main\")\n",
        "    .find_element_by_class_name(\"search-marvel-srp\")\n",
        "    .find_element_by_tag_name(\"h1\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vuDtYUK2IBQ"
      },
      "outputs": [],
      "source": [
        "text = infos_element.text\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do_VaYDH2IBQ"
      },
      "outputs": [],
      "source": [
        "if \"Search results page\" in text:\n",
        "    nums = int(text.split(\"(共\")[1].split(\" \")[1])\n",
        "else:\n",
        "    nums = int(text.split(\".\")[2].split(\"total\")[-1].split(\" \")[1])\n",
        "print(f\"number: {nums}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "jP55p2Jn2IBQ"
      },
      "outputs": [],
      "source": [
        "# Number of currently loaded pages + 1\n",
        "containers.get_property(\"childElementCount\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UWlVZvQ2IBQ"
      },
      "outputs": [],
      "source": [
        "# pages = int(math.ceil(nums / 10))\n",
        "pages = 9\n",
        "while pages + 1 != containers.get_property(\"childElementCount\"):\n",
        "    print(\n",
        "        f\"pages numbers：{pages}，cur page count: {int(containers.get_property('childElementCount'))-1}\"\n",
        "    )\n",
        "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    time.sleep(3)\n",
        "browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jprO6l252IBR"
      },
      "source": [
        "### Crawl target content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "FPAUM21J2IBR"
      },
      "outputs": [],
      "source": [
        "datas = get_target_contents(browser)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "dbnPc3Ch2IBR"
      },
      "outputs": [],
      "source": [
        "# show crawled data\n",
        "df = pd.DataFrame(datas)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY2PH1Da2IBR"
      },
      "source": [
        "### write to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "W5tBhTM32IBR"
      },
      "outputs": [],
      "source": [
        "file_name = \"res\"\n",
        "write_to_file(datas, file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcTRnUHt2IBR"
      },
      "source": [
        "### quit browser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s7kk4pw2IBR"
      },
      "outputs": [],
      "source": [
        "browser.quit()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "LinkedIn Post Scraping.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wApdwVxC2IBO",
        "xTt2nfMS2IBO",
        "hwOaGE0q2IBP",
        "jprO6l252IBR",
        "mY2PH1Da2IBR",
        "pcTRnUHt2IBR"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}